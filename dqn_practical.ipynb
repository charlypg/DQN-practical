{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e98c86c",
   "metadata": {},
   "source": [
    "# Deep Reinforcement Learning practical\n",
    "\n",
    "For some problems, formulated as Markov decision processes (MDP), the number of states and/or actions is very large, or even uncountable (continuous case). Such problems can not be solved by tabular RL methods, but often present an underlying structure or at least some continuity, making possible the use of approximative methods to approach the optimal value function. For more details, please refer to the monument written by Sutton and Bartho: *Reinforcement Learning: an Introduction, Part II*.\n",
    "\n",
    "The **goal** of this practical is to introduce modern tools that allow to build the current state-of-the-art deep reinforcement learning (deep RL) algorithms. We will start with a very simple example: linear regression. Then, because we only have an hour and a half, you will play with the hyperparameters of the famous [Deep Q-learning algorithm (DQN)](https://arxiv.org/pdf/1312.5602). In this notebook, the DQN agent has to learn how to control a simple simulated [lunar lander](https://gymnasium.farama.org/environments/box2d/lunar_lander/).\n",
    "\n",
    "Many RL algorithms derive from DQN. This notebook provides a standalone implementation, making it easy to implement extensions, variants or even new methods by a simple *inheritage* or *composition* (search Object-oriented programming for more information). Therefore, if you choose this practical for your **graded project**, you will implement and test an existing algorithm that derives from DQN, and provide some analysis of the results you have obtained.\n",
    "\n",
    "**Project instructions:**\n",
    "\n",
    "For your graded project, please focus on **ONE** of the options below. Also, we recommand to **ask questions** and **exchange with the supervisor** before you implement your variant. Your personal implementation will inherit from a single base class. You will only need to identify this class, choose which methods to override, and add hyperparameters if necessary. Please provide a .pdf with a detailed description of your project and your\n",
    "code (notebook or .py). In your project, feel free to come up with your own ideas, scientific questions and modeling choices but please detail them. Provide a comparison with DQN, explain the theoretical differences between algorithms and how it affects the results.\n",
    "\n",
    "- [QR-DQN](https://arxiv.org/pdf/1710.10044)\n",
    "- [DQN + Prioritized Experience Replay](https://arxiv.org/pdf/1511.05952)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526bae12",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    if \"google.colab\" in str(get_ipython()):\n",
    "        print('Running on Google Colab.')\n",
    "        !pip install swig\n",
    "        !pip install gymnasium==0.29.1\n",
    "        !pip install gymnasium[box2d]\n",
    "    else:\n",
    "        print(\"Not running on Google Colab. No install.\")\n",
    "except:\n",
    "    print(\"Not running on Google Colab. No install.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e000d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b61869b",
   "metadata": {},
   "source": [
    "## 1) Linear regression with JAX\n",
    "\n",
    "In this part, we use linear regression as a pretext to introduce JAX, an open source library developed by Google that provide all the necessary elements to build deep learning algorithms:\n",
    "\n",
    "- Numpy-like tensor manipulations\n",
    "- Parallel computation\n",
    "- Automatic differentiation\n",
    "\n",
    "As the parallel development of multiple disciplines have made deep learning possible, such as signal processing or physics, JAX can be used in other contexts than deep learning.\n",
    "\n",
    "Also, you will see that JAX, contrary to PyTorch or Tensorflow, follows a functional paradigm. It has some advantages in terms of computing but also understanding, as the written program will be close to the written maths."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a16f575",
   "metadata": {},
   "source": [
    "### Dataset generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26226dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_mixture(key, weights, mus, sigmas, shape):\n",
    "    \"\"\"\n",
    "    Draw samples from a Gaussian mixture distribution.\n",
    "    \n",
    "    weights: array of shape (K,) that sum to 1\n",
    "    mus: array of shape (K,)\n",
    "    sigmas: array of shape (K,)\n",
    "    shape: output shape, e.g. (n,)\n",
    "    \"\"\"\n",
    "    K = len(weights)\n",
    "    key_cat, key_norm = jax.random.split(key)\n",
    "\n",
    "    # Step 1: choose mixture component for each sample\n",
    "    comp_ids = jax.random.categorical(\n",
    "        key_cat,\n",
    "        jnp.log(weights),\n",
    "        shape=shape\n",
    "    )\n",
    "\n",
    "    # Step 2: sample Gaussians\n",
    "    eps = jax.random.normal(key_norm, shape)\n",
    "\n",
    "    # Gather mu_k and sigma_k for each sample\n",
    "    mu = mus[comp_ids]\n",
    "    sigma = sigmas[comp_ids]\n",
    "\n",
    "    return mu + sigma * eps\n",
    "\n",
    "def generate_weird_data(key: jnp.ndarray, n=500):\n",
    "    key1, key2, key3 = jax.random.split(key, 3)\n",
    "\n",
    "    # Linear component\n",
    "    x = jnp.linspace(-3, 3, n)\n",
    "\n",
    "    # # Nonlinear twist + heteroscedastic + fat-tailed noise\n",
    "    # noise_heavy = jax.random.t(df=10, key=key1)  # Student-t\n",
    "    # noise = (0.3 + 0.15 * x**2) * noise_heavy\n",
    "\n",
    "    # Mixture parameters\n",
    "    weights = jnp.array([0.5, 0.4, 0.1])\n",
    "    mus     = jnp.array([0, 2.0, -3.0])\n",
    "    sigmas  = jnp.array([2.0, 1.2, 0.4])*10\n",
    "\n",
    "    # heteroscedastic mixture noise\n",
    "    noise = gaussian_mixture(key1, weights, mus, sigmas, (n,))\n",
    "    noise = noise * (0.4 + 0.1 * x**2)\n",
    "\n",
    "    y = 2.0 * x + 1.0 + 0.8*jnp.sin(2*x) + noise\n",
    "\n",
    "    # Add a few big outliers\n",
    "    idx = jax.random.choice(key2, n, (10,), replace=False)\n",
    "    y = y.at[idx].set(y[idx] + jax.random.normal(key3, (10,)) * 10)\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c9b51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In JAX we need to propagate a key for pseudo-random number generation\n",
    "key = jax.random.PRNGKey(0)\n",
    "\n",
    "# Then we generate our date according to some weird distribution\n",
    "x, y = generate_weird_data(key)\n",
    "print(\"x:\", x.shape, x.dtype)\n",
    "print(\"y:\", y.shape, y.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb53a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x, y, marker=\"+\")\n",
    "ax.set_xlabel(r\"$x$\")\n",
    "ax.set_ylabel(r\"$y$\")\n",
    "ax.set_title(\"Dataset\")\n",
    "# ax.legend()\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ed1e1c",
   "metadata": {},
   "source": [
    "### Linear Least Squares (LLS)\n",
    "\n",
    "The $(x_i, y_i)$ datapoints seem to follow a linear tendency. As a result, we propose a linear model to describe our data on average. Thus we want to minimize the sum of squares: $\\mathcal{L}(a, b) = \\sum_i (a x_i + b - y_i )^2$ to find the parameters $(a, b)$ of the linear model.\n",
    "\n",
    "#### i) Analytical solution\n",
    "\n",
    "This problem has an analytical solution. If $x = (x_1, ..., x_n)$ and $y = (y_1, ..., y_n)$ then: $\\mathcal{L}(a, b) = || a x + b - y ||_2^2$.\n",
    "\n",
    "Such a function is convex with respect to $a, b$. Therefore, its gradient with respect to $(a, b)$ is $0$ if and only if $(a, b)$ is the global minimum. This leads to the solving of a linear system of equations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beabc33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(x)\n",
    "y = np.array(y)\n",
    "A = np.zeros((2, 2))\n",
    "A[0, 0] = np.sum(np.square(x))\n",
    "A[0, 1] = np.sum(x)\n",
    "A[1, 0] = np.sum(x)\n",
    "A[1, 1] = x.shape[0]\n",
    "B = np.zeros((2,))\n",
    "B[0] = np.dot(x, y)\n",
    "B[1] = np.sum(y)\n",
    "params = np.linalg.inv(A) @ B \n",
    "print(\"[a, b] =\", params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb57621",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x, y, marker=\"+\")\n",
    "ax.plot(x, params[0]*x + params[1], label=\"Analytical\", color=\"red\")\n",
    "ax.set_xlabel(r\"$x$\")\n",
    "ax.set_ylabel(r\"$y$\")\n",
    "ax.set_title(\"Analytical LLS\")\n",
    "# ax.legend()\n",
    "fig.tight_layout()\n",
    "# fig.savefig(\"linear_regression.pdf\")\n",
    "# plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017cc1a7",
   "metadata": {},
   "source": [
    "#### ii) Iterative solving via gradient descent\n",
    "\n",
    "Some problems do not have an analytical solution but can be solved iteratively. Before using such methods, we can check whether they work in a case where the solution is known. \n",
    "\n",
    "To train deep learning models, we specify a loss function that our model should minimize given some data, and perform gradient descent to find optimal parameters. The process is always the same, even for a simple LLS regression.\n",
    "\n",
    "We define the loss function ```linear_least_square_loss```. **WARNING:** The first parameter is ```params``` over which the gradient will be computed. This function returns a tuple ```loss_value, metrics```. Loss value will effectively be used by JAX to compute the gradient while the metrics provide some information on the learning process.\n",
    "\n",
    "Function ```jax.grad``` (see JAX documentation) computes the gradient thanks to the backpropagation algorithm. The following function ```gradient_step``` encapsulates that operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bd6724",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_least_square_loss(\n",
    "    params: jnp.ndarray,\n",
    "    x: jnp.ndarray,\n",
    "    y: jnp.ndarray\n",
    ") -> Tuple[jnp.ndarray, dict]:\n",
    "    y_model = params[0] * x + params[1]\n",
    "    loss = jnp.mean(jnp.square(y_model - y))\n",
    "    return loss, {\"loss\": loss, \"a\": params[0], \"b\": params[1]}\n",
    "\n",
    "def gradient_step(\n",
    "    params: jnp.ndarray,\n",
    "    x: jnp.ndarray,\n",
    "    y: jnp.ndarray,\n",
    "    step: float\n",
    ") -> Tuple[jnp.ndarray, dict]:\n",
    "    grad, metrics = jax.grad(linear_least_square_loss, has_aux=True)(\n",
    "        params, x, y\n",
    "    )\n",
    "    return params - step * grad, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96a2092",
   "metadata": {},
   "source": [
    "**Question 1:** Implement a for loop that performs $100$ steps of gradient descent with a gradient step of 1e-1 (two lines)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1498af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-1\n",
    "nb_steps = 100\n",
    "params_gd = jnp.zeros((2,))\n",
    "# BEGIN SOLUTION\n",
    "raise NotImplementedError\n",
    "# END SOLUTION\n",
    "print(metrics)\n",
    "print(\"[a, b] =\", params_gd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909e9903",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x, y, marker=\"+\")\n",
    "ax.plot(x, params[0]*x + params[1], label=\"Analytical\", color=\"red\")\n",
    "ax.plot(x, params_gd[0]*x + params_gd[1], label=\"Simple GD\", color=\"purple\")\n",
    "ax.set_xlabel(r\"$x$\")\n",
    "ax.set_ylabel(r\"$y$\")\n",
    "ax.set_title(\"Analytical vs Simple gradient descent\")\n",
    "ax.legend()\n",
    "fig.tight_layout()\n",
    "# fig.savefig(\"linear_regression.pdf\")\n",
    "# plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411a696e",
   "metadata": {},
   "source": [
    "To prevent people from reinventing the wheel, some packages exist around JAX, such as ```optax``` that implements many existing algorithms and routines for optimization. Therefore, we can propose another code based on ```optax```. For optimization, we choose the [Adam optimizer](https://arxiv.org/abs/1412.6980) which in this specific case does not really have sense, because Adam has been developed for stochastic optimization, but why not. The optimization process itself has an internal state that allows to take the loss curvature into account when performing the descent. Fortunately, we don't have to know these internal parameters to perform a descent. We simply memorize them via a variable ```opt_state```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bcd4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "optimizer = optax.adam(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75288f3a",
   "metadata": {},
   "source": [
    "The following cell performs Adam gradient descent with ```optax```.\n",
    "\n",
    "Algorithm:\n",
    "- Initialize the optimizer state: ```opt_state = optimizer.init(params_gd_adams)```\n",
    "- For ```nb_steps```:\n",
    "    - Compute the gradient of the loss with ```jax.grad```\n",
    "    - Compute the parameter updates an the new optimizer state ```updates, opt_state = optimizer.update(grad, opt_state, params_gd_adams)```\n",
    "    - Use ```optax.apply_updates``` to apply parameters modifications.\n",
    "\n",
    "**Don't hesitate to take a look at the ```optax``` documentation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279c2c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "params_gd_adams = jnp.zeros((2,))\n",
    "opt_state = optimizer.init(params_gd_adams)\n",
    "for _ in range(nb_steps):\n",
    "    grad, metrics = jax.grad(linear_least_square_loss, has_aux=True)(\n",
    "        params_gd_adams, x, y\n",
    "    )\n",
    "    updates, opt_state = optimizer.update(grad, opt_state, params_gd_adams)\n",
    "    params_gd_adams = optax.apply_updates(params_gd_adams, updates)\n",
    "print(metrics)\n",
    "print(\"[a, b] =\", params_gd_adams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83fe466",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x, y, marker=\"+\")\n",
    "ax.plot(x, params[0]*x + params[1], label=\"Analytical\", color=\"red\")\n",
    "ax.plot(x, params_gd[0]*x + params_gd[1], label=\"Simple GD\", color=\"purple\")\n",
    "ax.plot(x, params_gd_adams[0]*x + params_gd_adams[1], label=\"Adam GD\", color=\"green\")\n",
    "ax.set_xlabel(r\"$x$\")\n",
    "ax.set_ylabel(r\"$y$\")\n",
    "ax.set_title(\"Comparison of the three methods\")\n",
    "ax.legend()\n",
    "fig.tight_layout()\n",
    "# fig.savefig(\"linear_regression.pdf\")\n",
    "# plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29819b3",
   "metadata": {},
   "source": [
    "#### Conclusion on LLS\n",
    "\n",
    "In this case, the number of points is reasonable making possible the analytical solving of the LLS problem. If we had way more datapoints, computing the analytical solution would be almost impossible. Also, for many problems, like image classification, we don't even know the analytical solution. However, computing a loss and its gradient with respect to the parameters of our model is easy. In deep learning we compute the gradient over batches of data to approximate the direction of descent. This process is called Stochastic Gradient Descent (SGD). Adam is a state-of-the-art algorithm in the SGD family.\n",
    "\n",
    "The LLS solving via gradient descent is very interesting as it is very simple but uses the same tools as complex deep learning learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a4b280",
   "metadata": {},
   "source": [
    "### Quantile regression\n",
    "\n",
    "Instead of finding the line that passes on average through all the data points, we will find the line that splits our data into two parts. One part will contain a certain proportion of the data, and the other will contain the remaining points. This is known as quantile regression.\n",
    "\n",
    "Below you will find the loss associated to quantile regression. Then, taking inspiration from the LLS case, you will implement the gradient descent algorithm with ```optax```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10928be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def huber(td: jnp.ndarray) -> jnp.ndarray:\n",
    "    \"\"\"Huber function.\"\"\"\n",
    "    abs_td = jnp.abs(td)\n",
    "    return jnp.where(abs_td <= 1.0, jnp.square(td), abs_td)\n",
    "\n",
    "def quantile_loss(\n",
    "    params: jnp.ndarray,\n",
    "    x: jnp.ndarray,\n",
    "    y: jnp.ndarray,\n",
    "    expectile: float\n",
    ") -> Tuple[jnp.ndarray, dict]:\n",
    "    y_model = params[0] * x + params[1]\n",
    "    difference = y - y_model\n",
    "    element_wise_loss = huber(difference)\n",
    "    element_wise_loss *= jax.lax.stop_gradient(\n",
    "        jnp.abs(expectile - (difference < 0))\n",
    "    )\n",
    "    loss = jnp.mean(element_wise_loss)\n",
    "    return loss, {\"loss\": loss, \"a\": params[0], \"b\": params[1]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab9fc1f",
   "metadata": {},
   "source": [
    "**Question 2:** Implement the gradient descent algorithm using ```optax```. (Few lines)\n",
    "\n",
    "Take inspiration from LLS and use the same ```optimizer```.\n",
    "\n",
    "*Don't hesitate to take a look at the ```optax``` documentation.*\n",
    "\n",
    "**Question 3:** Execute the code with ```expectile``` equal to 0.9. Then play with the value to see what happens on the plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bd327c",
   "metadata": {},
   "outputs": [],
   "source": [
    "expectile = 0.9\n",
    "assert expectile > 0 and expectile < 1.0\n",
    "nb_steps = 1_000\n",
    "params_qr = jnp.zeros((2,))\n",
    "# BEGIN SOLUTION\n",
    "raise NotImplementedError\n",
    "# END SOLUTION\n",
    "print(metrics)\n",
    "print(\"[a, b] =\", params_qr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d287c3ab",
   "metadata": {},
   "source": [
    "**Question 4:** How far is $a$ from LLS $a$ ? How about $b$ ? Why ? What is the meaning of ```expectile``` ? Why should it lie between 0 and 1 ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46e8b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x, y, marker=\"+\")\n",
    "ax.plot(x, params_qr[0]*x + params_qr[1], label=r\"QR ($\\tau = {}$)\".format(expectile), color=\"red\")\n",
    "ax.set_xlabel(r\"$x$\")\n",
    "ax.set_ylabel(r\"$y$\")\n",
    "ax.set_title(\"Linear Quantile Regression\")\n",
    "ax.legend()\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d8ff8e",
   "metadata": {},
   "source": [
    "#### Conclusion on Linear Quantile Regression\n",
    "\n",
    "Quantile regression is very useful when we are interested in finding \"best cases\" or \"worst cases\". For example, with expectile $0.99$, we know that 99% of our points lie under the line.\n",
    "\n",
    "Least squares are theoretically related to the expectation, which is why it is widely used in RL. Quantile regression is at the core of quantile-based distributional RL methods (QR-DQN, TQC, Worst-case SAC, etc). This family of methods can capture more information on the distribution of the return, allowing to solve worst-case problems or tackle overestimation.\n",
    "\n",
    "In both cases, a gradient step over a regression loss is performed to reduce a Bellman error, which is why we studied a simple linear regression example in this practical."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67dd065",
   "metadata": {},
   "source": [
    "### General conclusion on JAX \n",
    "\n",
    "What we have seen:\n",
    "- LLS and linear quantile regression\n",
    "- Automatic differentiation of a loss function in JAX\n",
    "- Gradient descent and its implementation with optax.\n",
    "\n",
    "- Note that whether a model is linear or not—even a neural network—the code for gradient updates, and often even for the loss functions, remains almost identical. \n",
    "\n",
    "What we have not covered:\n",
    "- JAX just-in-time compilation to accelerate computation (jax.jit)\n",
    "- JAX parallelization (jax.vmap, jax.pmap)\n",
    "- Neural network implementation (Flax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800860e7",
   "metadata": {},
   "source": [
    "## 2) Deep Q-learning\n",
    "\n",
    "In this part, we propose to play with an implementation of [DQN](https://arxiv.org/abs/1312.5602) and [DoubleDQN](https://arxiv.org/abs/1509.06461). The agent has to solve the [Lunar Lander v2](https://gymnasium.farama.org/environments/box2d/lunar_lander/) environment. \n",
    "\n",
    "As the computation time is about 10 minutes, different hyperparameters will be assigned to different groups. Then we will draw some comparisons among groups to highlight the effect of different hyperparameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453e6994",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import functools\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.flatten_util import ravel_pytree\n",
    "import flax\n",
    "import flax.linen as nn\n",
    "import optax\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from typing import Any, Callable, Dict, Optional, Sequence, Tuple, Union"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a82cd56",
   "metadata": {},
   "source": [
    "### Buffer and Sampler (Nothing to do)\n",
    "\n",
    "The replay buffer (Buffer class) stores the transitions during training. At each gradient step, a batch of transitions is sampled from the replay buffer (Sampler class). In the DQN class (a few cells below), the gradient over parameters is computed for each batch item. The mean over these gradient gives the orientation of the minimization (See [SGD](https://en.wikipedia.org/wiki/Stochastic_gradient_descent)).\n",
    "\n",
    "\n",
    "The following cell is adapted from [**xpag**](https://github.com/perrin-isir/xpag), an RL library developed by Nicolas Perrin-Gilbert et al.\n",
    "\n",
    "Copyright (c) 2022-2023, CNRS - Licensed under BSD 3-Clause License.\n",
    "\n",
    "**Take a look if interested.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff26f89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This file is adapted from xpag\n",
    "# Original source: https://github.com/perrin-isir/xpag/xpag/buffers/buffer.py\n",
    "# Commit: fad4d9cf77053cf29b42f322091bdf182012a501\n",
    "# Copyright (c) 2022-2023, CNRS - Licensed under BSD 3-Clause License.\n",
    "\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from enum import Enum\n",
    "\n",
    "class DataType(Enum):\n",
    "    NUMPY = \"data represented as numpy arrays\"\n",
    "    JAX = \"data represented as jax.numpy arrays\"\n",
    "\n",
    "\n",
    "def get_datatype(x: Union[np.ndarray, jnp.ndarray]) -> DataType:\n",
    "    if isinstance(x, jnp.ndarray):\n",
    "        return DataType.JAX\n",
    "    elif isinstance(x, np.ndarray):\n",
    "        return DataType.NUMPY\n",
    "    else:\n",
    "        raise TypeError(f\"{type(x)} not handled.\")\n",
    "\n",
    "\n",
    "def datatype_convert(\n",
    "    x: Union[np.ndarray, jnp.ndarray, list, float],\n",
    "    datatype: Union[DataType, None] = DataType.NUMPY,\n",
    ") -> Union[np.ndarray, jnp.ndarray]:\n",
    "    if datatype is None:\n",
    "        return x\n",
    "    elif datatype == DataType.NUMPY:\n",
    "        if isinstance(x, np.ndarray):\n",
    "            return x\n",
    "        else:\n",
    "            return np.array(x)\n",
    "    elif datatype == DataType.JAX:\n",
    "        if isinstance(x, jnp.ndarray):\n",
    "            return x\n",
    "        else:\n",
    "            return jnp.array(x)\n",
    "\n",
    "\n",
    "class Sampler(ABC):\n",
    "    def __init__(self, *, seed: Union[int, None] = None):\n",
    "        self.seed = seed\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def sample(\n",
    "        self,\n",
    "        buffer,\n",
    "        batch_size: int,\n",
    "    ) -> Dict[str, Union[np.ndarray, jnp.ndarray]]:\n",
    "        \"\"\"Return a batch of transitions\"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class DefaultSampler(Sampler):\n",
    "    def __init__(self, *, seed: Union[int, None] = None):\n",
    "        super().__init__(seed=seed)\n",
    "\n",
    "    def sample(\n",
    "        self,\n",
    "        buffer: Dict[str, Union[np.ndarray]],\n",
    "        batch_size: int,\n",
    "    ) -> Dict[str, Union[np.ndarray]]:\n",
    "        buffer_size = next(iter(buffer.values())).shape[0]\n",
    "        idxs = self.rng.choice(\n",
    "            buffer_size,\n",
    "            size=batch_size,\n",
    "            replace=True,\n",
    "        )\n",
    "        transitions = {key: buffer[key][idxs] for key in buffer.keys()}\n",
    "        return transitions\n",
    "\n",
    "\n",
    "class Buffer(ABC):\n",
    "    \"\"\"Base class for buffers\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        buffer_size: int,\n",
    "        sampler: Optional[Sampler] = None,\n",
    "    ):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.sampler = sampler\n",
    "\n",
    "    @abstractmethod\n",
    "    def insert(self, step: Dict[str, Any]):\n",
    "        \"\"\"Inserts a transition in the buffer\"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def sample(self, batch_size) -> Dict[str, Union[np.ndarray, jnp.ndarray]]:\n",
    "        \"\"\"Uses the sampler to returns a batch of transitions\"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class DefaultBuffer(Buffer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        buffer_size: int,\n",
    "        sampler: Sampler,\n",
    "    ):\n",
    "        super().__init__(buffer_size, sampler)\n",
    "        self.current_size = 0\n",
    "        self.buffers = {}\n",
    "        self.size = buffer_size\n",
    "        self.dict_sizes = None\n",
    "        self.num_envs = None\n",
    "        self.keys = None\n",
    "        self.zeros = None\n",
    "        self.where = None\n",
    "        self.first_insert_done = False\n",
    "\n",
    "    def init_buffer(self, step: Dict[str, Any]):\n",
    "        self.dict_sizes = {}\n",
    "        self.keys = list(step.keys())\n",
    "        assert \"terminated\" in self.keys\n",
    "        for key in self.keys:\n",
    "            if isinstance(step[key], dict):\n",
    "                for k in step[key]:\n",
    "                    assert len(step[key][k].shape) == 2\n",
    "                    self.dict_sizes[key + \".\" + k] = step[key][k].shape[1]\n",
    "            else:\n",
    "                assert len(step[key].shape) == 2\n",
    "                self.dict_sizes[key] = step[key].shape[1]\n",
    "        self.num_envs = step[\"terminated\"].shape[0]\n",
    "        for key in self.dict_sizes:\n",
    "            self.buffers[key] = np.zeros([self.size, self.dict_sizes[key]])\n",
    "        self.zeros = lambda i: np.zeros(i).astype(\"int\")\n",
    "        self.where = np.where\n",
    "        self.first_insert_done = True\n",
    "\n",
    "    def insert(self, step: Dict[str, Any]):\n",
    "        if not self.first_insert_done:\n",
    "            self.init_buffer(step)\n",
    "        idxs = self._get_storage_idx(inc=self.num_envs)\n",
    "        for key in self.keys:\n",
    "            if isinstance(step[key], dict):\n",
    "                for k in step[key]:\n",
    "                    self.buffers[key + \".\" + k][idxs, :] = datatype_convert(\n",
    "                        step[key][k], DataType.NUMPY\n",
    "                    ).reshape((self.num_envs, self.dict_sizes[key + \".\" + k]))\n",
    "            else:\n",
    "                self.buffers[key][idxs, :] = datatype_convert(\n",
    "                    step[key], DataType.NUMPY\n",
    "                ).reshape((self.num_envs, self.dict_sizes[key]))\n",
    "\n",
    "    def pre_sample(self):\n",
    "        temp_buffers = {}\n",
    "        for key in self.buffers.keys():\n",
    "            temp_buffers[key] = self.buffers[key][: self.current_size]\n",
    "        return temp_buffers\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return self.sampler.sample(self.pre_sample(), batch_size)\n",
    "\n",
    "    def _get_storage_idx(self, inc=None):\n",
    "        inc = inc or 1\n",
    "        if self.current_size + inc <= self.size:\n",
    "            idx = np.arange(self.current_size, self.current_size + inc)\n",
    "        elif self.current_size < self.size:\n",
    "            overflow = inc - (self.size - self.current_size)\n",
    "            idx_a = np.arange(self.current_size, self.size)\n",
    "            idx_b = np.random.randint(0, self.current_size, overflow)\n",
    "            idx = np.concatenate([idx_a, idx_b])\n",
    "        else:\n",
    "            idx = np.random.randint(0, self.size, inc)\n",
    "        self.current_size = min(self.size, self.current_size + inc)\n",
    "        return idx\n",
    "\n",
    "    def save(self, directory: str):\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "        list_vars = [\n",
    "            (\"current_size\", self.current_size),\n",
    "            (\"buffers\", self.buffers),\n",
    "            (\"size\", self.size),\n",
    "            (\"dict_sizes\", self.dict_sizes),\n",
    "            (\"num_envs\", self.num_envs),\n",
    "            (\"keys\", self.keys),\n",
    "            (\"first_insert_done\", self.first_insert_done),\n",
    "        ]\n",
    "        for cpl in list_vars:\n",
    "            with open(os.path.join(directory, cpl[0] + \".joblib\"), \"wb\") as f_:\n",
    "                joblib.dump(cpl[1], f_)\n",
    "\n",
    "    def load(self, directory: str):\n",
    "        self.current_size = joblib.load(os.path.join(directory, \"current_size.joblib\"))\n",
    "        self.buffers = joblib.load(os.path.join(directory, \"buffers.joblib\"))\n",
    "        self.size = joblib.load(os.path.join(directory, \"size.joblib\"))\n",
    "        self.dict_sizes = joblib.load(os.path.join(directory, \"dict_sizes.joblib\"))\n",
    "        self.num_envs = joblib.load(os.path.join(directory, \"num_envs.joblib\"))\n",
    "        self.keys = joblib.load(os.path.join(directory, \"keys.joblib\"))\n",
    "        self.first_insert_done = joblib.load(\n",
    "            os.path.join(directory, \"first_insert_done.joblib\")\n",
    "        )\n",
    "        self.zeros = lambda i: np.zeros(i).astype(\"int\")\n",
    "        self.where = np.where"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29368ed7",
   "metadata": {},
   "source": [
    "### From Multi-Layer Perceptron (MLP) to Deep Q-network (Nothing to do)\n",
    "\n",
    "Since the state of our environment is represented as a vector rather than an image or any other high dimensional input, the deep Q-network reduces to a standard multi-layer perceptron (MLP). It consists of several hidden layers and produces one output per possible action.\n",
    "\n",
    "**Take a look if interested.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933517fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def default_init(scale: Optional[float] = jnp.sqrt(2)):\n",
    "    return nn.initializers.orthogonal(scale)\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    hidden_dims: Sequence[int]\n",
    "    activations: Callable[[jnp.ndarray], jnp.ndarray] = nn.relu\n",
    "    activate_final: int = False\n",
    "    init_last_layer_zeros: bool = False\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x: jnp.ndarray) -> jnp.ndarray:\n",
    "        for i, size in enumerate(self.hidden_dims):\n",
    "            if i + 1 == len(self.hidden_dims) and self.init_last_layer_zeros:\n",
    "                x = nn.Dense(\n",
    "                    size,\n",
    "                    kernel_init=nn.initializers.zeros_init(),\n",
    "                    bias_init=nn.initializers.zeros_init(),\n",
    "                )(x)\n",
    "            else:\n",
    "                x = nn.Dense(size, kernel_init=default_init())(x)\n",
    "            if i + 1 < len(self.hidden_dims) or self.activate_final:\n",
    "                x = self.activations(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DeepQNetwork(nn.Module):\n",
    "    action_dim: int\n",
    "    hidden_dims: Sequence[int]\n",
    "    activations: Callable[[jnp.ndarray], jnp.ndarray] = nn.relu\n",
    "    activate_final: int = False\n",
    "    init_last_layer_zeros: bool = False\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, observations: jnp.ndarray) -> jnp.ndarray:\n",
    "        action_values = MLP(\n",
    "            hidden_dims=(*self.hidden_dims, self.action_dim),\n",
    "            activations=self.activations,\n",
    "            activate_final=self.activate_final,\n",
    "            init_last_layer_zeros=self.init_last_layer_zeros\n",
    "        )(observations)\n",
    "        return action_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7125bb7",
   "metadata": {},
   "source": [
    "### DQN and DoubleDQN (Nothing to do)\n",
    "\n",
    "The DQN class represents a DQN agent. It can initialize the Q-network parameters, act based on received observation, and learn from batches of transitions.\n",
    " \n",
    "The only difference between DoubleDQN and DQN is the loss, which is why the DoubleDQN class inherits from the DQN class.\n",
    "\n",
    "**If interested:** take a look from the bottom to the top of the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5a21c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Params = flax.core.FrozenDict[str, Any]\n",
    "Metrics = Dict[str, Any]\n",
    "\n",
    "def random_key_split(rng: jnp.ndarray) -> Tuple[jnp.ndarray, jnp.ndarray]:\n",
    "    return jax.random.split(rng)\n",
    "\n",
    "def init(\n",
    "    model_def: nn.Module,\n",
    "    inputs: Sequence[jnp.ndarray],\n",
    "    tx: Optional[optax.GradientTransformation] = None,\n",
    ") -> Tuple[Params, optax.OptState]:\n",
    "    \"\"\"Generic initialization function\"\"\"\n",
    "    # Initialize params\n",
    "    variables = model_def.init(*inputs)\n",
    "    params = variables.pop(\"params\")\n",
    "\n",
    "    # Initialize optimizer state\n",
    "    if tx is not None:\n",
    "        opt_state = tx.init(params)\n",
    "    else:\n",
    "        opt_state = None\n",
    "\n",
    "    return params, opt_state\n",
    "\n",
    "def grad_norm(grad):\n",
    "    flattened_grads, _ =  ravel_pytree(grad)\n",
    "    return jax.numpy.linalg.norm(flattened_grads)\n",
    "\n",
    "def get_shapes(params: Params):\n",
    "    return jax.tree_util.tree_map(lambda p: p.shape, params)\n",
    "\n",
    "def apply_updates(\n",
    "    optimizer: optax.GradientTransformation,\n",
    "    grad: Params, \n",
    "    opt_state: optax.OptState, \n",
    "    params: Params,\n",
    ") -> Tuple[Params, optax.OptState, Params]:\n",
    "    updates, new_opt_state = optimizer.update(grad, opt_state, params)\n",
    "    new_params = optax.apply_updates(params, updates)\n",
    "    return new_params, new_opt_state, updates\n",
    "\n",
    "def ema_target_update(\n",
    "    params: Params, target_params: Params, tau: float\n",
    ") -> Params:\n",
    "    new_target_params = jax.tree_util.tree_map(\n",
    "        lambda p, tp: p * tau + tp * (1 - tau), params, target_params\n",
    "    )\n",
    "\n",
    "    return new_target_params\n",
    "\n",
    "def huber(td: jnp.ndarray) -> jnp.ndarray:\n",
    "    \"\"\"Huber function.\"\"\"\n",
    "    abs_td = jnp.abs(td)\n",
    "    return jnp.where(abs_td <= 1.0, jnp.square(td), abs_td)\n",
    "\n",
    "\n",
    "class DQN:\n",
    "    \"\"\"Class implementing a DQN algorithm.\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        observation_dim: int,\n",
    "        action_dim: int,\n",
    "        gamma: float = 0.99,\n",
    "        learning_rate = 3e-4,\n",
    "        update_target_every_x_steps: int = 10_000,\n",
    "        network_features: dict = dict(hidden_dims=(128, 128)),\n",
    "        eps_greedy_hp: dict = dict(\n",
    "            eps_decrease=\"exponential\",\n",
    "            eps_end=0.01,\n",
    "            eps_end_at=4e5\n",
    "        )\n",
    "    ):\n",
    "        # Observation and action spaces\n",
    "        self.observation_dim = observation_dim\n",
    "        self.action_dim = action_dim\n",
    "\n",
    "        # Discount factor\n",
    "        self.gamma = gamma\n",
    "\n",
    "        # Epsilon-greedy parameters\n",
    "        self.eps_greedy_hp = eps_greedy_hp.copy()\n",
    "        if self.eps_greedy_hp[\"eps_decrease\"] == \"exponential\":\n",
    "            self.eps_dec = self.eps_greedy_hp[\"eps_end\"]**(\n",
    "                1 / self.eps_greedy_hp[\"eps_end_at\"]\n",
    "            )\n",
    "            print(\"DQN.eps_dec (exponential):\", self.eps_dec)\n",
    "            self.update_epsilon = self.update_epsilon_exponential\n",
    "        elif self.eps_greedy_hp[\"eps_decrease\"] == \"linear\":\n",
    "            self.eps_dec = (\n",
    "                self.eps_greedy_hp[\"eps_end\"] - 1.0\n",
    "            ) / self.eps_greedy_hp[\"eps_end_at\"]\n",
    "            print(\"DQN.eps_dec (linear):\", self.eps_dec)\n",
    "            self.update_epsilon = self.update_epsilon_linear\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        # Optimization\n",
    "        self.optimizer = optax.adam(learning_rate=learning_rate)\n",
    "        self.update_target_every_x_steps = update_target_every_x_steps\n",
    "\n",
    "        # Neural network\n",
    "        self.network = self.make_network(network_features)\n",
    "    \n",
    "    def make_network(self, network_features: dict) -> DeepQNetwork:\n",
    "        return DeepQNetwork(\n",
    "            action_dim=self.action_dim,\n",
    "            **network_features\n",
    "        )\n",
    "    \n",
    "    def init(self, rng: jnp.ndarray) -> Tuple[Params, optax.OptState, Params, float]:\n",
    "        init_obs = jnp.ones((1, 1, self.observation_dim))\n",
    "\n",
    "        params, opt_state = init(\n",
    "            model_def=self.network,\n",
    "            inputs=[rng, init_obs],\n",
    "            tx=self.optimizer\n",
    "        )\n",
    "        print(get_shapes(params))\n",
    "\n",
    "        target_params, _ = init(\n",
    "            model_def=self.network,\n",
    "            inputs=[rng, init_obs]\n",
    "        )\n",
    "        print(get_shapes(target_params))\n",
    "\n",
    "        epsilon = 1.0\n",
    "        print(\"epsilon:\", epsilon)\n",
    "\n",
    "        return params, opt_state, target_params, epsilon\n",
    "    \n",
    "    @functools.partial(\n",
    "        jax.jit,\n",
    "        static_argnames=(\n",
    "            \"self\",\n",
    "        )\n",
    "    )\n",
    "    def select_action(\n",
    "        self,\n",
    "        params: Params,\n",
    "        observation: jnp.ndarray\n",
    "    ) -> jnp.ndarray:\n",
    "        print(\"COMPILE: DQN.select_action\")\n",
    "        actions_values = self.network.apply({\"params\": params}, observation)\n",
    "        return jnp.argmax(actions_values, axis=-1)\n",
    "    \n",
    "    def select_single_action(\n",
    "        self,\n",
    "        params: Params,\n",
    "        observation: np.ndarray\n",
    "    ) -> jnp.ndarray:\n",
    "        return int(\n",
    "            jnp.squeeze(\n",
    "                self.select_action(\n",
    "                    params=params,\n",
    "                    observation=jnp.expand_dims(jnp.array(observation), axis=0)\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    def dqn_loss(\n",
    "        self,\n",
    "        params: Params,\n",
    "        target_params: Params,\n",
    "        observations: jnp.ndarray,\n",
    "        actions: jnp.ndarray,\n",
    "        next_observations: jnp.ndarray,\n",
    "        rewards: jnp.ndarray,\n",
    "        masks: jnp.ndarray\n",
    "    ) -> Tuple[jnp.ndarray, Metrics]:\n",
    "        actions_values = self.network.apply({\"params\": params}, observations)\n",
    "        actions = jnp.array(actions, dtype=jnp.int32)\n",
    "        current_action_values = jnp.take_along_axis(\n",
    "            actions_values, actions, axis=-1\n",
    "        ).squeeze(axis=-1)\n",
    "        next_actions_values = self.network.apply({\"params\": target_params}, next_observations)\n",
    "        next_values = jnp.max(next_actions_values, axis=-1)\n",
    "        rewards = jnp.squeeze(rewards, axis=-1)\n",
    "        masks = jnp.squeeze(masks, axis=-1)\n",
    "        target_values = rewards + self.gamma * masks * next_values\n",
    "        print(rewards.shape, masks.shape, next_values.shape)\n",
    "        print(current_action_values.shape, target_values.shape)\n",
    "        batch_loss = huber(current_action_values - target_values)\n",
    "        loss = jnp.mean(batch_loss)\n",
    "        return loss, {\n",
    "            \"loss\": loss,\n",
    "            \"q_mean\": jnp.mean(current_action_values),\n",
    "            \"next_q_mean\": jnp.mean(next_values)\n",
    "        }\n",
    "    \n",
    "    @functools.partial(\n",
    "        jax.jit,\n",
    "        static_argnames=(\n",
    "            \"self\",\n",
    "        )\n",
    "    )\n",
    "    def gradient_step(\n",
    "        self,\n",
    "        params: Params,\n",
    "        opt_state: optax.OptState,\n",
    "        target_params: Params,\n",
    "        batch: Dict[str, np.ndarray]\n",
    "    ) -> tuple:\n",
    "        print(\"COMPILE: DQN.gradient_step\")\n",
    "        grad, metrics = jax.grad(self.dqn_loss, has_aux=True)(\n",
    "            params,\n",
    "            target_params=target_params,\n",
    "            observations=batch[\"observation\"],\n",
    "            actions=batch[\"action\"],\n",
    "            next_observations=batch[\"next_observation\"],\n",
    "            rewards=batch[\"reward\"],\n",
    "            masks=1-batch[\"terminated\"]\n",
    "        )\n",
    "        params, opt_state, updates = apply_updates(\n",
    "            optimizer=self.optimizer,\n",
    "            grad=grad,\n",
    "            opt_state=opt_state,\n",
    "            params=params\n",
    "        )\n",
    "        return params, opt_state, updates, grad, metrics\n",
    "    \n",
    "    def update_epsilon_exponential(self, epsilon: float) -> float:\n",
    "        return max(self.eps_greedy_hp[\"eps_end\"], self.eps_dec * epsilon)\n",
    "    \n",
    "    def update_epsilon_linear(self, epsilon: float) -> float:\n",
    "        return max(self.eps_greedy_hp[\"eps_end\"], self.eps_dec + epsilon)\n",
    "    \n",
    "    def update_target_params(\n",
    "        self,\n",
    "        params: Params,\n",
    "        target_params: Params,\n",
    "        step: int\n",
    "    ) -> Params:\n",
    "        if step % self.update_target_every_x_steps == 0:\n",
    "            return params\n",
    "        else:\n",
    "            return target_params\n",
    "    \n",
    "    def update(\n",
    "        self,\n",
    "        params: Params,\n",
    "        opt_state: optax.OptState,\n",
    "        target_params: Params,\n",
    "        epsilon: float,\n",
    "        batch: Dict[str, np.ndarray],\n",
    "        step: int\n",
    "    ) -> tuple:\n",
    "        # Perform gradient descent step\n",
    "        params, opt_state, updates, grad, metrics = self.gradient_step(\n",
    "            params=params,\n",
    "            opt_state=opt_state,\n",
    "            target_params=target_params,\n",
    "            batch=batch\n",
    "        )\n",
    "\n",
    "        # Update epsilon\n",
    "        epsilon = self.update_epsilon(epsilon)\n",
    "\n",
    "        # Update target params\n",
    "        target_params = self.update_target_params(\n",
    "            params=params,\n",
    "            target_params=target_params,\n",
    "            step=step\n",
    "        )\n",
    "\n",
    "        return params, opt_state, target_params, epsilon, metrics\n",
    "\n",
    "\n",
    "class DoubleDQN(DQN):\n",
    "    def dqn_loss(\n",
    "        self,\n",
    "        params: Params,\n",
    "        target_params: Params,\n",
    "        observations: jnp.ndarray,\n",
    "        actions: jnp.ndarray,\n",
    "        next_observations: jnp.ndarray,\n",
    "        rewards: jnp.ndarray,\n",
    "        masks: jnp.ndarray\n",
    "    ) -> Tuple[jnp.ndarray, Metrics]:\n",
    "        # Compute current value\n",
    "        actions_values = self.network.apply({\"params\": params}, observations)\n",
    "        actions = jnp.array(actions, dtype=jnp.int32)\n",
    "        current_action_values = jnp.take_along_axis(\n",
    "            actions_values, actions, axis=-1\n",
    "        ).squeeze(axis=-1)\n",
    "        # Compute next actions according to current q function\n",
    "        next_actions_values = jax.lax.stop_gradient(\n",
    "            self.network.apply({\"params\": params}, next_observations)\n",
    "        )\n",
    "        next_actions = jnp.argmax(next_actions_values, axis=-1, keepdims=True)\n",
    "        # Compute target values according to https://arxiv.org/pdf/1509.06461\n",
    "        target_next_actions_values = self.network.apply(\n",
    "            {\"params\": target_params}, next_observations\n",
    "        )\n",
    "        next_values = jnp.take_along_axis(\n",
    "            target_next_actions_values, next_actions, axis=-1\n",
    "        ).squeeze(axis=-1)\n",
    "        rewards = jnp.squeeze(rewards, axis=-1)\n",
    "        masks = jnp.squeeze(masks, axis=-1)\n",
    "        target_values = rewards + self.gamma * masks * next_values\n",
    "        print(rewards.shape, masks.shape, next_values.shape)\n",
    "        print(current_action_values.shape, target_values.shape)\n",
    "        # Compute loss\n",
    "        batch_loss = huber(current_action_values - target_values)\n",
    "        loss = jnp.mean(batch_loss)\n",
    "        return loss, {\n",
    "            \"loss\": loss,\n",
    "            \"q_mean\": jnp.mean(current_action_values),\n",
    "            \"next_q_mean\": jnp.mean(next_values)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6fcf11",
   "metadata": {},
   "source": [
    "### Logging and Evaluation (Nothing to do)\n",
    "\n",
    "Just a simple pandas logger. Data can then be plotted and/or saved in a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8a1216",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PandasLogger:\n",
    "    def __init__(self):\n",
    "        self.df = pd.DataFrame()\n",
    "    \n",
    "    @property\n",
    "    def data(self):\n",
    "        return self.df\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.df.__str__()\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.df.__repr__()\n",
    "    \n",
    "    def log(self, data: dict):\n",
    "        if type(data) != dict:\n",
    "            raise ValueError(\"log() requires a dict\")\n",
    "\n",
    "        if self.df.empty and len(self.df.columns) == 0:\n",
    "            self.df = pd.DataFrame(columns=data.keys())\n",
    "        \n",
    "        self.df = pd.concat([self.df, pd.DataFrame([data])], ignore_index=True)\n",
    "    \n",
    "    def save(self, filepath: str):\n",
    "        \"\"\"\n",
    "        Save internal dataframe to a CSV file.\n",
    "        \"\"\"\n",
    "        filepath = Path(filepath)\n",
    "        filepath.parent.mkdir(parents=True, exist_ok=True)\n",
    "        self.df.to_csv(filepath, index=False)\n",
    "    \n",
    "    def load(self, filepath: str):\n",
    "        \"\"\"\n",
    "        Load data from a CSV file into the internal dataframe.\n",
    "        \"\"\"\n",
    "        filepath = Path(filepath)\n",
    "        if not filepath.exists():\n",
    "            raise FileNotFoundError(f\"No file found at {filepath}\")\n",
    "\n",
    "        self.df = pd.read_csv(filepath)\n",
    "\n",
    "def eval_episode(\n",
    "    eval_env: gym.Env,\n",
    "    # model_def: DeepQNetwork,\n",
    "    agent: DQN,\n",
    "    params: Params\n",
    ") -> float:\n",
    "    obs, info = eval_env.reset()\n",
    "    done = False\n",
    "    sum_of_rewards = 0\n",
    "    while not(done):\n",
    "        action = agent.select_single_action(params, obs)\n",
    "        next_obs, reward, term, trunc, info = eval_env.step(action)\n",
    "        sum_of_rewards += reward\n",
    "\n",
    "        done = term or trunc\n",
    "        obs = next_obs\n",
    "    return sum_of_rewards\n",
    "\n",
    "def eval(\n",
    "    eval_env: gym.Env,\n",
    "    agent: DQN,\n",
    "    params: Params,\n",
    "    nb_episodes: int\n",
    ") -> Metrics:\n",
    "    returns = np.zeros((nb_episodes,))\n",
    "    for i in range(nb_episodes):\n",
    "        returns[i] = eval_episode(\n",
    "            eval_env=eval_env, agent=agent, params=params\n",
    "        )\n",
    "    return {\n",
    "        \"return_mean\": np.mean(returns),\n",
    "        \"return_median\": np.median(returns),\n",
    "        \"return_25\": np.quantile(returns, 0.25),\n",
    "        \"return_75\": np.quantile(returns, 0.75)\n",
    "    }\n",
    "\n",
    "def plot_from_dataframe(\n",
    "    fig, ax,\n",
    "    df: pd.DataFrame,\n",
    "    x_key: str, y_key: str,\n",
    "    fill_between: Optional[Tuple[str, str]] = None,\n",
    "    xlabel: Optional[str] = None,\n",
    "    ylabel: Optional[str] = None\n",
    "):\n",
    "    x = pd.to_numeric(df[x_key], errors=\"coerce\").to_numpy()\n",
    "    y = pd.to_numeric(df[y_key], errors=\"coerce\").to_numpy()\n",
    "    \n",
    "    if fill_between is not None:\n",
    "        ymin = pd.to_numeric(df[fill_between[0]], errors=\"coerce\").to_numpy()\n",
    "        ymax = pd.to_numeric(df[fill_between[1]], errors=\"coerce\").to_numpy()\n",
    "        ax.fill_between(x, ymin, ymax, alpha=0.2)\n",
    "    \n",
    "    ax.plot(x, y)\n",
    "    ax.grid()\n",
    "\n",
    "    if xlabel is not None:\n",
    "        ax.set_xlabel(xlabel)\n",
    "    if ylabel is not None:\n",
    "        ax.set_ylabel(ylabel)\n",
    "\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca09d2e",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "**Question 5:** Identify the hyperparameters of DQN. What do they mean ? How do you think they influence the training ?\n",
    "\n",
    "**Question 6:** Call the teacher and launch the training when ready.\n",
    "\n",
    "**Question 7:** What is the difference between DQN and DoubleDQN ? Of course the loss... But what is different in the loss ?\n",
    "\n",
    "**Question 8:** How data is sampled from the replay buffer ? How new data is inserted ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e7a25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment\n",
    "env_kwargs = {\n",
    "    \"LunarLander-v2\": dict(\n",
    "        max_episode_steps=1_000,\n",
    "        continuous=False,\n",
    "        gravity=-9.81,\n",
    "        enable_wind=False, \n",
    "        wind_power=15.0,\n",
    "        turbulence_power=1.5\n",
    "    )\n",
    "}\n",
    "env_name = \"LunarLander-v2\"\n",
    "env = gym.make(\n",
    "    env_name,\n",
    "    render_mode=None,\n",
    "    **env_kwargs[env_name]\n",
    ")\n",
    "num_eval_episodes = 20\n",
    "if num_eval_episodes == 1:\n",
    "    eval_render_mode = \"human\"\n",
    "else:\n",
    "    eval_render_mode = None\n",
    "eval_env = gym.make(\n",
    "    env_name,\n",
    "    render_mode=eval_render_mode,\n",
    "    **env_kwargs[env_name]\n",
    ")\n",
    "observation_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "# JAX random state\n",
    "seed = 0\n",
    "np_rand_state = np.random.RandomState(seed)\n",
    "rng = jax.random.PRNGKey(seed)\n",
    "\n",
    "# Agent\n",
    "# agent = DQN(\n",
    "agent = DoubleDQN(\n",
    "    observation_dim=observation_dim, action_dim=action_dim,\n",
    "    gamma=0.999,\n",
    "    learning_rate=1e-3,\n",
    "    update_target_every_x_steps=5_000,\n",
    "    network_features=dict(hidden_dims=(256, 256)),\n",
    "    eps_greedy_hp = dict(\n",
    "        eps_decrease=\"exponential\",\n",
    "        eps_end=0.01,\n",
    "        eps_end_at=4.5e5\n",
    "    )\n",
    ")\n",
    "rng, sub = jax.random.split(rng)\n",
    "params, opt_state, target_params, epsilon = agent.init(sub)\n",
    "\n",
    "# Buffer and sampler\n",
    "buffer_size = 200_000\n",
    "buffer = DefaultBuffer(\n",
    "    buffer_size=buffer_size,\n",
    "    sampler=DefaultSampler()\n",
    ")\n",
    "\n",
    "# Main loop\n",
    "batch_size = 64\n",
    "max_steps = 400_100\n",
    "start_training_after_x_steps = 10_000\n",
    "eval_every_x_steps = 50_000\n",
    "update_params_every = 4\n",
    "\n",
    "logger = PandasLogger()\n",
    "metrics = None\n",
    "observation, info = env.reset()\n",
    "for step in tqdm(range(max_steps)):\n",
    "    # Evaluation\n",
    "    if step % eval_every_x_steps == 0:\n",
    "        eval_metrics = eval(\n",
    "            eval_env=eval_env,\n",
    "            agent=agent,\n",
    "            params=params,\n",
    "            nb_episodes=num_eval_episodes\n",
    "        )\n",
    "        if metrics is not None:\n",
    "            logger.log({\"step\": step, \"epsilon\": epsilon, **eval_metrics, **metrics})\n",
    "            print(\n",
    "                \"step:\", step, \";\",\n",
    "                \"epsilon: {:.2f}\".format(epsilon), \";\",\n",
    "                \"return_median: {:.3f}\".format(eval_metrics[\"return_median\"]), \";\",\n",
    "                \"return_25: {:.3f}\".format(eval_metrics[\"return_25\"]), \";\",\n",
    "                \"return_75: {:.3f}\".format(eval_metrics[\"return_75\"]), \";\",\n",
    "                \"loss:\", metrics[\"loss\"],\n",
    "                \"q_mean:\", metrics[\"q_mean\"],\n",
    "                \"next_q_mean:\", metrics[\"next_q_mean\"]\n",
    "            )\n",
    "        else:\n",
    "            logger.log({\"step\": step, \"epsilon\": epsilon, **eval_metrics})\n",
    "            print(\n",
    "                \"step:\", step, \";\",\n",
    "                \"epsilon: {:.2f}\".format(epsilon), \";\",\n",
    "                \"return_median: {:.3f}\".format(eval_metrics[\"return_median\"]), \";\",\n",
    "                \"return_25: {:.3f}\".format(eval_metrics[\"return_25\"]), \";\",\n",
    "                \"return_75: {:.3f}\".format(eval_metrics[\"return_75\"]), \";\"\n",
    "            )\n",
    "\n",
    "    # Select action\n",
    "    if np_rand_state.uniform() < epsilon:\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        action = agent.select_single_action(params, observation)\n",
    "    \n",
    "    # Perform step\n",
    "    next_observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    # Store transition\n",
    "    transition = {\n",
    "        \"observation\": np.expand_dims(observation, axis=0),\n",
    "        \"action\": np.array([[action]], dtype=np.int64),\n",
    "        \"next_observation\": np.expand_dims(next_observation, axis=0),\n",
    "        \"reward\": np.array([[reward]]),\n",
    "        \"terminated\": np.array([[terminated]])\n",
    "    }\n",
    "    buffer.insert(transition)\n",
    "\n",
    "    # Update if necessary\n",
    "    if step > start_training_after_x_steps:\n",
    "        # Sample batch\n",
    "        batch = buffer.sample(batch_size)\n",
    "\n",
    "        if step % update_params_every == 0:\n",
    "            # Perform gradient descent step\n",
    "            params, opt_state, updates, grad, metrics = agent.gradient_step(\n",
    "                params=params,\n",
    "                opt_state=opt_state,\n",
    "                target_params=target_params,\n",
    "                batch=batch\n",
    "            )\n",
    "        # Update target params\n",
    "        target_params = agent.update_target_params(\n",
    "            params=params,\n",
    "            target_params=target_params,\n",
    "            step=step\n",
    "        )\n",
    "        # Update epsilon\n",
    "        epsilon = agent.update_epsilon(epsilon)\n",
    "\n",
    "    # Prepare next iter\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "    else:\n",
    "        observation = next_observation\n",
    "\n",
    "# Save learning\n",
    "logger.save(\"./crash_test.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4f8d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some figures\n",
    "df = logger.data\n",
    "fig, ax = plt.subplots()\n",
    "fig, ax = plot_from_dataframe(\n",
    "    fig, ax, df,\n",
    "    x_key=\"step\", y_key=\"return_median\",\n",
    "    fill_between=(\"return_25\", \"return_75\"),\n",
    "    xlabel=\"Step\",\n",
    "    ylabel=\"Return\"\n",
    ")\n",
    "fig.tight_layout()\n",
    "ax.plot()\n",
    "# fig.savefig(\"return.pdf\")\n",
    "# plt.close(fig)\n",
    "\n",
    "for y_key in df.keys():\n",
    "    if y_key != \"step\" and not(\"return\" in y_key):\n",
    "        fig, ax = plt.subplots()\n",
    "        fig, ax = plot_from_dataframe(\n",
    "            fig, ax, df,\n",
    "            x_key=\"step\", y_key=y_key,\n",
    "            xlabel=\"Step\",\n",
    "            ylabel=y_key\n",
    "        )\n",
    "        fig.tight_layout()\n",
    "        ax.plot()\n",
    "        # fig.savefig(\"{}.pdf\".format(y_key))\n",
    "        # plt.close(fig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "toydistrib",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
