# DQN-practical
A practical introduction to deep reinforcement learning for beginners.

The notebook is composed of 2 parts. In the first, you will implement different linear regression methods with JAX. The second part provides an implementation of [DQN](https://arxiv.org/pdf/1312.5602) and [DoubleDQN](https://arxiv.org/pdf/1509.06461), so that you can vary hyperparameters. 

Then you can choose ONE **project** in which you will implement a variant of the DQN method, either QR-DQN or PER+DQN.

## Project 1: QR-DQN

In the first part of the practical we have seen multiple ways of performing linear regression with JAX and two types of regression: least-square regression and quantile regression. [DQN](https://arxiv.org/pdf/1312.5602) (*already implemented*) and [DoubleDQN](https://arxiv.org/pdf/1509.06461) (*already implemented*) seek to minimize the square error between the state-action value function $Q(s, a ; \theta)$ and its target $r + \gamma (1 - \text{term})\max_{a'} Q(s', a' ; \overline{\theta})$, so as to estimate the expected return of a deterministic optimal policy $\pi$: $\mathbb{E}_{\pi}\left[\sum_t \gamma^t r_t \right]$. $\theta$ are the Q-network parameters, and $\overline{\theta}$ the target parameters that follow $\theta$.

However, the expectation has many defaults, among which its sensitivity with respect to extreme values that may occult the complexity of the environment (See [Will Dabney â€“ Advances in Distributional Reinforcement Learning And Connections With Planning](https://www.youtube.com/watch?v=iqIGHSgYtbs)).
The return itself $\sum_t \gamma^t r_t$ can be seen as a real random variable. Like any real random variable it has a probability distribution $\mathcal{D}_{\pi}\left[\sum_t \gamma^t r_t \right]$ that can be represented under the form of a density function or of a cumulative distribution function. Distributional reinforcement learning methods seek to approximate the return distribution, either density, moments or cumulative form. 

Let $Z = \sum_t \gamma^t r_t$. The cumulative distribution function is defined as followed: $F_Z(z) \triangleq \mathbb{P}(Z \le z)$. As a result, there is a string connection between the cumulative distribution and quantiles. For example, if $F_Z(q) = \mathbb{P}(Z \le q) = 0.25$, then $q$ is the first quartile. If $F_Z(q) = \mathbb{P}(Z \le q) = 0.5$, then $q$ is the second quartile, *aka* the median. If $F_Z(q) = \mathbb{P}(Z \le q) = 0.75$, then $q$ is the third quartile. Thus, if we can estimate those metrics, we have a better vue of our distribution, can infer more things and have more information to optimize our policy. For instance, some methods are interested in worst-case scenarios (see [WCPG](https://proceedings.mlr.press/v100/tang20a/tang20a.pdf)). In short the RL algorithm has to find the policy that maximizes the low value quantiles instead of the expectation.

[Quantile Regression DQN (QR-DQN)](https://arxiv.org/pdf/1710.10044) estimates quantiles of the return distribution $\mathcal{D}_{\pi}\left[\sum_t \gamma^t r_t \right]$. For expectile $\tau \in ]0, 1[$, the associated quantile $q_{\tau} \in \mathbb{R}$ verifies: $\tau = \mathbb{P}(Z \le q_{\tau}) = F_Z(q_{\tau})$. In short, QR-DQN approximates $N$, in increasing order: $(q_{\tau_0}, q_{\tau_1}, ..., q_{\tau_i}, ..., q_{\tau_{N-1}})$, where: $\forall i \in [\![0, N-1]\!], q_{\tau_i} = \frac{2 i+1}{2 N}$ .

### Differences with DQN
- The neural network of QR-DQN has outputs $N \times |\mathcal{A}|$ outputs: $N$ quantiles per possible action $a$: $(q_{\tau_0}(s, a; \theta), q_{\tau_1}(s, a; \theta), ..., q_{\tau_i}(s, a; \theta), ..., q_{\tau_{N-1}}(s, a; \theta))$, whereas in DQN the neural network outputs $|\mathcal{A}|$ values corresponding to the number of possible actions. 

- The selected action at each step is $\pi(s) = \arg \max_a \frac{1}{N} \sum_i q_{\tau_i}(s, a; \theta)$ instead of $\pi(s) = \arg \max_a Q(s, a; \theta)$.

- Instead of squared error, QR-DQN computes the following loss:
$$\mathcal{L}(\theta) = \sum_{i=0}^{N-1} \frac{1}{N} \sum_{j=0}^{N-1} \rho_{\tau_i}\left(\text{target}_j - q_{\tau_i}(s, a; \theta)\right)$$
Where: $\text{target}_j = r + \gamma (1 - \text{term}) q_{\tau_j}(s', a^*; \overline{\theta})$ and $a^* = \arg \max_a \frac{1}{N} \sum_i q_{\tau_j}(s', a; \overline{\theta})$.

### TODO

As a result, the only thing to change, or copy paste, in the notebook is the agent class: the DQN class. You can implement a derived class ```QR-DQN``` so that you implement only the aspects that differs from DQN. To do so, you can follow the steps after.


1. Copy-paste the ```DeepQNetwork``` class and rename it  ```QuantileNetwork```. Add a hyperparameter ```num_quantiles```, which is an integer. In ```hidden_dims```, replace ```self.action_dim``` with ```self.action_dim * num_quantiles``` so that you now have a network with an output of shape ```(batch_size, action_dim * num_quantiles)```. Before the return, add a reshape to obtain an output of shape ```(batch_size, action_dim, num_quantiles)```. In a separate cell you can test your code by instantiating the network, giving some outputs, check the shapes, etc.

2. Implement a derived agent class: ```class QR-DQN(DQN)```. Copy-paste the ```__init__``` method signature, add ```num_quantiles``` as an input. Use ```DQN.__init__(self, other arguments...)``` to not copy-paste everything. Also, copy-paste the ```make_network``` method and modify it to create a ```QuantileNetwork``` instead.

3. Copy-paste ```select_action``` method and modify.

4. Copy-paste ```dqn_loss``` method and modify.

5. Modify the agent before the training loop. 

6. Train your agent. Get the learning curve and the CSV file so that you can compare your results with what you have obtained with DQN during the practical.

7. If you have time... use multiple seeds.

During the process you can use any ressource you may need, including LLMs, but you have to show what you have learnt, understood, what you have not understood, comment your results. 

**IMPORTANT remarks:** Don't hesitate to send your questions. If [Lunar-Lander](https://gymnasium.farama.org/environments/box2d/lunar_lander/) is too slow to train, use [Cartpole](https://gymnasium.farama.org/environments/classic_control/cart_pole/) and train it over 1e5 or even less.


## Project 2: Prioritized Experience Replay (PER)

TODO

## Related repositories

- [RL-Atelier](https://github.com/charlypg/RL-Atelier)
- [xpag](https://github.com/perrin-isir/xpag/) (Buffer and Sampler classes, see CREDITS.md for more information)
